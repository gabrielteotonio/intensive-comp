area0_id = "59138b2103205749cdfba80a",
seed = 2002,
period = 7)
geoEpid::simulate_seir(od_matrix_normal$od_matrix_partition,
od_matrix_normal$pop_df,
area0_id = "59138b2103205749cdfba80a",
seed = 2002,
period = 7)
geoEpid::simulate_seir
simulate_seir_debug <- function (od_matrix, pop_df, n_days = 300, area0_id, area0_infected = 10,
beta = 0.75, gamma = 0.3, sigma = 0.2, seed = 2019, period = 1,
first_period_day = -1)
{
set.seed(seed)
population <- pop_df$population
population_sum <- sum(population)
areas_number <- nrow(pop_df)
sigma_vec <- rep(sigma, areas_number)
gamma_vec <- rep(gamma, areas_number)
beta_vec <- rep(beta, areas_number)
SEIR <- matrix(nrow = areas_number, ncol = 4)
colnames(SEIR) <- c("S", "E", "I", "R")
SEIR[, "S"] <- population
SEIR[, "E"] <- 0
SEIR[, "I"] <- 0
SEIR[, "R"] <- 0
first_infections <- (pop_df$id == area0_id) * area0_infected
SEIR[, "S"] <- SEIR[, "S"] - first_infections
SEIR[, "I"] <- SEIR[, "I"] + first_infections
SEIR_normalized <- SEIR/rowSums(SEIR)
SEIR_normalized[is.na(SEIR_normalized)] <- 0
SEIR_simulation <- SEIR
SEIR_normalized_simulation <- SEIR_normalized
print(paste0("Checking if total population match...", sum(SEIR_simulation) ==
sum(pop_df$population)))
susceptible_pop_norm <- vector()
exposed_pop_norm <- vector()
infected_pop_norm <- vector()
recovered_pop_norm <- vector()
susceptible_pop_norm <- c(susceptible_pop_norm, sum(SEIR[,
"S"])/population_sum)
exposed_pop_norm <- c(exposed_pop_norm, sum(SEIR[, "E"])/population_sum)
infected_pop_norm <- c(infected_pop_norm, sum(SEIR[, "I"])/population_sum)
recovered_pop_norm <- c(recovered_pop_norm, sum(SEIR[, "R"])/population_sum)
infected_by_day <- list()
infected_by_day[[1]] <- SEIR_simulation[, "I"]
day <- 0
period_day <- first_period_day
total_inflow <- 1
total_new_exposed <- 1
print("Starting simulation")
while (total_new_exposed > 0 && day < n_days) {
day <- day + 1
print(paste0("Day: ", day))
OD <- if (period == 1)
od_matrix
else od_matrix[, , period_day + 1]
period_day <- (period_day + 1)%%period
infected_mat <- replicate(areas_number, SEIR_normalized_simulation[,
"I"])
OD_infected <- round(OD * infected_mat)
inflow_infected <- colSums(OD_infected)
total_inflow_infected <- sum(inflow_infected)
print(paste0("Total infected inflow: ", total_inflow_infected))
new_exposed <- beta_vec * SEIR_simulation[, "S"] * inflow_infected/(population +
colSums(OD)) + beta_vec * SEIR_simulation[, "S"] *
SEIR_simulation[, "I"]/population
new_exposed[is.na(new_exposed)] <- 0
total_new_exposed <- round(sum(new_exposed))
print(paste0("New exposed: ", total_new_exposed))
new_exposed <- ifelse(new_exposed > SEIR_simulation[,
"S"], SEIR_simulation[, "S"], new_exposed)
new_infected <- sigma_vec * SEIR_simulation[, "E"]
total_new_infected <- round(sum(new_infected, na.rm = T))
print(paste0("New infected: ", total_new_infected))
new_recovered <- gamma_vec * SEIR_simulation[, "I"]
total_new_recovered <- round(sum(new_recovered, na.rm = T))
print(paste0("New recovered: ", total_new_recovered))
SEIR_simulation[, "S"] <- SEIR_simulation[, "S"] - new_exposed
SEIR_simulation[, "E"] <- SEIR_simulation[, "E"] + new_exposed -
new_infected
SEIR_simulation[, "I"] <- SEIR_simulation[, "I"] + new_infected -
new_recovered
SEIR_simulation[, "R"] <- SEIR_simulation[, "R"] + new_recovered
SEIR_simulation <- ifelse(SEIR_simulation < 0, 0, SEIR_simulation)
SEIR_normalized_simulation <- SEIR_simulation/rowSums(SEIR_simulation)
SEIR_normalized_simulation[is.na(SEIR_normalized_simulation)] <- 0
S <- sum(SEIR_simulation[, "S"])/population_sum
E <- sum(SEIR_simulation[, "E"])/population_sum
I <- sum(SEIR_simulation[, "I"])/population_sum
R <- sum(SEIR_simulation[, "R"])/population_sum
print(paste("S:", scales::percent(S), "E:", scales::percent(E),
"I:", scales::percent(I), "R:", scales::percent(R),
"Total:", population_sum, sep = " "))
susceptible_pop_norm <- c(susceptible_pop_norm, S)
exposed_pop_norm <- c(exposed_pop_norm, E)
infected_pop_norm <- c(infected_pop_norm, I)
recovered_pop_norm <- c(recovered_pop_norm, R)
infected_by_day[[day + 1]] <- round(SEIR_simulation[,
"I"])
}
df_pop_norm <- data.frame(day = 1:(day + 1), s_p = susceptible_pop_norm,
e_p = exposed_pop_norm, i_p = infected_pop_norm, r_p = recovered_pop_norm)
print(paste0("total infected: ", scales::percent(1 - tail(df_pop_norm$s_p,
1))))
simulation_output <- list(od_matrix = od_matrix, beta = beta,
gamma = gamma, sigma = sigma, area0_id = area0_id, area0_infected = area0_infected,
curves = df_pop_norm, infected_by_day = infected_by_day,
seed = seed)
print("The simulation has end")
return(simulation_output)
}
simulate_seir_debug <- function (od_matrix, pop_df, n_days = 300, area0_id, area0_infected = 10,
beta = 0.75, gamma = 0.3, sigma = 0.2, seed = 2019, period = 1,
first_period_day = -1)
{
set.seed(seed)
population <- pop_df$population
population_sum <- sum(population)
areas_number <- nrow(pop_df)
sigma_vec <- rep(sigma, areas_number)
gamma_vec <- rep(gamma, areas_number)
beta_vec <- rep(beta, areas_number)
SEIR <- matrix(nrow = areas_number, ncol = 4)
colnames(SEIR) <- c("S", "E", "I", "R")
SEIR[, "S"] <- population
SEIR[, "E"] <- 0
SEIR[, "I"] <- 0
SEIR[, "R"] <- 0
first_infections <- (pop_df$id == area0_id) * area0_infected
SEIR[, "S"] <- SEIR[, "S"] - first_infections
SEIR[, "I"] <- SEIR[, "I"] + first_infections
SEIR_normalized <- SEIR/rowSums(SEIR)
SEIR_normalized[is.na(SEIR_normalized)] <- 0
SEIR_simulation <- SEIR
SEIR_normalized_simulation <- SEIR_normalized
print(paste0("Checking if total population match...", sum(SEIR_simulation) ==
sum(pop_df$population)))
susceptible_pop_norm <- vector()
exposed_pop_norm <- vector()
infected_pop_norm <- vector()
recovered_pop_norm <- vector()
susceptible_pop_norm <- c(susceptible_pop_norm, sum(SEIR[,
"S"])/population_sum)
exposed_pop_norm <- c(exposed_pop_norm, sum(SEIR[, "E"])/population_sum)
infected_pop_norm <- c(infected_pop_norm, sum(SEIR[, "I"])/population_sum)
recovered_pop_norm <- c(recovered_pop_norm, sum(SEIR[, "R"])/population_sum)
infected_by_day <- list()
infected_by_day[[1]] <- SEIR_simulation[, "I"]
day <- 0
period_day <- first_period_day
total_inflow <- 1
total_new_exposed <- 1
print("Starting simulation")
while (total_new_exposed > 0 && day < n_days) {
day <- day + 1
print(paste0("Day: ", day))
OD <- if (period == 1)
od_matrix
else od_matrix[, , period_day + 1]
period_day <- (period_day + 1)%%period
infected_mat <- replicate(areas_number, SEIR_normalized_simulation[,
"I"])
print(dim(OD))
print(dim(infected_mat))
OD_infected <- round(OD * infected_mat)
inflow_infected <- colSums(OD_infected)
total_inflow_infected <- sum(inflow_infected)
print(paste0("Total infected inflow: ", total_inflow_infected))
new_exposed <- beta_vec * SEIR_simulation[, "S"] * inflow_infected/(population +
colSums(OD)) + beta_vec * SEIR_simulation[, "S"] *
SEIR_simulation[, "I"]/population
new_exposed[is.na(new_exposed)] <- 0
total_new_exposed <- round(sum(new_exposed))
print(paste0("New exposed: ", total_new_exposed))
new_exposed <- ifelse(new_exposed > SEIR_simulation[,
"S"], SEIR_simulation[, "S"], new_exposed)
new_infected <- sigma_vec * SEIR_simulation[, "E"]
total_new_infected <- round(sum(new_infected, na.rm = T))
print(paste0("New infected: ", total_new_infected))
new_recovered <- gamma_vec * SEIR_simulation[, "I"]
total_new_recovered <- round(sum(new_recovered, na.rm = T))
print(paste0("New recovered: ", total_new_recovered))
SEIR_simulation[, "S"] <- SEIR_simulation[, "S"] - new_exposed
SEIR_simulation[, "E"] <- SEIR_simulation[, "E"] + new_exposed -
new_infected
SEIR_simulation[, "I"] <- SEIR_simulation[, "I"] + new_infected -
new_recovered
SEIR_simulation[, "R"] <- SEIR_simulation[, "R"] + new_recovered
SEIR_simulation <- ifelse(SEIR_simulation < 0, 0, SEIR_simulation)
SEIR_normalized_simulation <- SEIR_simulation/rowSums(SEIR_simulation)
SEIR_normalized_simulation[is.na(SEIR_normalized_simulation)] <- 0
S <- sum(SEIR_simulation[, "S"])/population_sum
E <- sum(SEIR_simulation[, "E"])/population_sum
I <- sum(SEIR_simulation[, "I"])/population_sum
R <- sum(SEIR_simulation[, "R"])/population_sum
print(paste("S:", scales::percent(S), "E:", scales::percent(E),
"I:", scales::percent(I), "R:", scales::percent(R),
"Total:", population_sum, sep = " "))
susceptible_pop_norm <- c(susceptible_pop_norm, S)
exposed_pop_norm <- c(exposed_pop_norm, E)
infected_pop_norm <- c(infected_pop_norm, I)
recovered_pop_norm <- c(recovered_pop_norm, R)
infected_by_day[[day + 1]] <- round(SEIR_simulation[,
"I"])
}
df_pop_norm <- data.frame(day = 1:(day + 1), s_p = susceptible_pop_norm,
e_p = exposed_pop_norm, i_p = infected_pop_norm, r_p = recovered_pop_norm)
print(paste0("total infected: ", scales::percent(1 - tail(df_pop_norm$s_p,
1))))
simulation_output <- list(od_matrix = od_matrix, beta = beta,
gamma = gamma, sigma = sigma, area0_id = area0_id, area0_infected = area0_infected,
curves = df_pop_norm, infected_by_day = infected_by_day,
seed = seed)
print("The simulation has end")
return(simulation_output)
}
model_normal <- simulate_seir_debug(od_matrix_normal$od_matrix_partition,
od_matrix_normal$pop_df,
area0_id = "59138b2103205749cdfba80a",
seed = 2002,
period = 7)
version()
version
library(tidyverse)
install.packages("tidyverse")
install.packages("tidyverse")
remove.packages("tidyverse")
install.packages("tidyverse")
install.packages("stringi")
system(java -version)
system("java -version")
install.packages("sparklyr")
packageVersion(sparklyr)
packageVersion("sparklyr")
spark_all()
library(sparklyr)
spark_all()
spark_install()
sc <- spark_connect(master = "local", version = "2.3")
sc <- spark_connect(master = "local")
cars <- copy_to(sc, cars)
cars
*cars*
cars
cars
cars
spark_web(sc)
# Analysis -----
library(DBI)
dbGetQuery(sc, "SELECT count(*) FROM mtcars")
dbGetQuery(sc, "SELECT count(*) FROM cars")
cars <- copy_to(sc, mtcars)
cars
dbGetQuery(sc, "SELECT count(*) FROM mtcars")
library(dplyr)
count(mtcars)
select(cars, hp, mpg) %>%
sample_n(100) %>%
collect() %>%
plot()
model <- ml_linear_regression(cars, mpg ~hp)
model
data.frame(hp = 250 + 10 *1:10)
model %>%
ml_predict(copy_to(sc, data.frame(hp = 250 + 10 *1:10))) %>%
transmute(hp = hp, mpg = prediction) %>%
full_join(select(cars, hp, mpg)) %>%
collect() %>%
plot()
model %>%
ml_predict(copy_to(sc, data.frame(hp = 250 + 10 *1:10)))
model %>%
ml_predict(copy_to(sc, data.frame(hp = 250 + 10 *1:10))) %>%
transmute(hp = hp, mpg = prediction) %>%
full_join(select(cars, hp, mpg)) %>%
collect() %>%
plot()
spark_write_csv(cars, "cars.csv")
install.packages("sparklyr.nested")
library(sparklyr.nested)
sdf_nest(cars, hp) %>%
group_by(cyl) %>%
summarise(data = collect_list(data))
getwd()
# Streaming -----
dir.create("input")
write.csv(mtcars, "input/cars_1.csv", row.names = F)
stream <- stream_read_csv(sc, "input/") %>%
select(mpg, cyl, disp) %>%
stream_write_csv("output/")
dir("output", pattern = ".csv")
stream <- stream_read_csv(sc, "input/") %>%
select(mpg, cyl, disp) %>%
stream_write_csv("output/")
stream_stop(stream)
spark_disconnect(sc)
system()
system
Sys.info()
library(tidyverse)
library(dplyr)
library(ggplot2)
library(getPass)
library(RPresto)
library(DBI)
library(httr)
theme_set(theme_bw(base_size = 15, base_family = "Monospace"))
# setwd("/path/to/working/dir")
# presto config
user = "gabriel.teotonio"; pswd = getPass("Password: ")
set_config(config(httpauth = 1, userpwd = paste(user, pswd, sep = ":")))
db = function(schema = "default") {
dbConnect(RPresto::Presto(), host = "https://main-presto.inloco.tech",
port = 443, user = user, schema = schema, catalog = "glue")
}
presto = function(query) as_tibble(dbGetQuery(db(), str_interp(query)))
# save job result to disk:
tbl(db("scratch"), "abel_av_visits") %>%
mutate(hashed_address_line = as.character(hashed_address_line)) %>%
collect() %>%
write_csv("visits-joined-claims.csv")
# save job result to disk:
tbl(db("scratch"), "abel_av_visits") %>%
mutate(hashed_address_line = as.character(hashed_address_line)) %>%
collect() %>%
write_csv("visits-joined-claims.csv")
library(readr)
# save job result to disk:
tbl(db("scratch"), "abel_av_visits") %>%
mutate(hashed_address_line = as.character(hashed_address_line)) %>%
collect() %>%
write_csv("visits-joined-claims.csv")
data
visits_joined_claims = read_csv("visits-joined-claims.csv") %>%
mutate(hashed_address_line = as.character(hashed_address_line)) %>%
select(hashed_mad_id, hashed_address_line, result, distance_bucket, visits) %>%
mutate(result = factor(result,
levels = c("no_data", "zero_matches", "low", "medium", "high"),
ordered = TRUE))
# conditional distributions of results
cond_dist = function(given, other) {
given = match.arg(given, c("result_prod", "inferred_result"))
other = match.arg(other, c("result_prod", "inferred_result"))
visits_joined_claims %>%
filter(distance_bucket %in% c(1001, 100)) %>%
mutate(distance_bucket = paste0("v", distance_bucket)) %>%
pivot_wider(names_from = "distance_bucket", values_from = "visits") %>%
mutate(inferred_result = factor(case_when(
is.na(v1001) ~ "no_data",
is.na(v100) ~ "zero_matches",
v100 < 4 ~ "low",
v100 == 4 ~ "medium",
TRUE ~ "high"
), levels = c("no_data", "zero_matches", "low", "medium", "high"), ordered = T)) %>%
count(inferred_result, result_prod = result) %>%
group_by_at(vars(given)) %>%
mutate(prob_other = scales::percent(n / sum(n), .01),
height_given = sum(n)) %>%
ungroup() %>%
mutate(prob_given = scales::percent(height_given / sum(n), .01)) %>%
ungroup() %>%
ggplot(aes_string(given, "n")) +
geom_col(fill = "lightgrey") +
geom_col(aes_string(fill = other), position = "dodge") +
geom_text(aes_string(group = other, label = "prob_other"),
family = "Monospace",
size = 3.5,
position = position_dodge(width = 0.9),
vjust = -0.25) +
geom_text(aes_string(y = "height_given", label = "prob_given"),
family = "Monospace",
size = 3.5,
vjust = -0.5) +
scale_y_continuous("Claims", scales::pretty_breaks(10)) +
scale_fill_brewer(name = other, palette = "Set2") +
facet_wrap(as.formula(paste("~", given)), scales = "free", nrow = 1) +
ggtitle(paste0("dist. of ", other, " given ", given))
}
p1 = cond_dist("result_prod", "inferred_result")
library(tidyr)
p1 = cond_dist("result_prod", "inferred_result")
p2
p1
# save job result to disk:
tbl(db("scratch"), "fastios_av_visits") %>%
mutate(hashed_address_line = as.character(hashed_address_line)) %>%
collect() %>%
write_csv("visits-joined-claims.csv")
data
visits_joined_claims = read_csv("visits-joined-claims.csv") %>%
mutate(hashed_address_line = as.character(hashed_address_line)) %>%
select(hashed_mad_id, hashed_address_line, result, distance_bucket, visits) %>%
mutate(result = factor(result,
levels = c("no_data", "zero_matches", "low", "medium", "high"),
ordered = TRUE))
# conditional distributions of results
cond_dist = function(given, other) {
given = match.arg(given, c("result_prod", "inferred_result"))
other = match.arg(other, c("result_prod", "inferred_result"))
visits_joined_claims %>%
filter(distance_bucket %in% c(1001, 100)) %>%
mutate(distance_bucket = paste0("v", distance_bucket)) %>%
pivot_wider(names_from = "distance_bucket", values_from = "visits") %>%
mutate(inferred_result = factor(case_when(
is.na(v1001) ~ "no_data",
is.na(v100) ~ "zero_matches",
v100 < 4 ~ "low",
v100 == 4 ~ "medium",
TRUE ~ "high"
), levels = c("no_data", "zero_matches", "low", "medium", "high"), ordered = T)) %>%
count(inferred_result, result_prod = result) %>%
group_by_at(vars(given)) %>%
mutate(prob_other = scales::percent(n / sum(n), .01),
height_given = sum(n)) %>%
ungroup() %>%
mutate(prob_given = scales::percent(height_given / sum(n), .01)) %>%
ungroup() %>%
ggplot(aes_string(given, "n")) +
geom_col(fill = "lightgrey") +
geom_col(aes_string(fill = other), position = "dodge") +
geom_text(aes_string(group = other, label = "prob_other"),
family = "Monospace",
size = 3.5,
position = position_dodge(width = 0.9),
vjust = -0.25) +
geom_text(aes_string(y = "height_given", label = "prob_given"),
family = "Monospace",
size = 3.5,
vjust = -0.5) +
scale_y_continuous("Claims", scales::pretty_breaks(10)) +
scale_fill_brewer(name = other, palette = "Set2") +
facet_wrap(as.formula(paste("~", given)), scales = "free", nrow = 1) +
ggtitle(paste0("dist. of ", other, " given ", given))
}
p1 = cond_dist("result_prod", "inferred_result")
p2 = cond_dist("inferred_result", "result_prod")
gridExtra::grid.arrange(p1, p2, nrow = 2)
# conditional distribution of claims per distance to claimed address given db result
visits_joined_claims %>%
group_by(hashed_mad_id, hashed_address_line, result) %>%
summarise(distance_bucket = min(distance_bucket)) %>%
ungroup() %>%
count(result, distance_bucket, name = "claims") %>%
group_by(result) %>%
arrange(distance_bucket) %>%
mutate(prop_claims = claims / sum(claims)) %>%
ungroup() %>%
mutate(label = scales::percent(prop_claims, .01)) %>%
filter(distance_bucket <= 500) %>%
ggplot(aes(factor(distance_bucket), prop_claims, label = label)) +
geom_col(fill = "grey") +
geom_text(vjust = -0.25, family = "Monospace") +
scale_y_continuous("% of claims given result", scales::pretty_breaks(10),
labels = scales::percent) +
xlab("distance_bucket") +
scale_color_manual(values = RColorBrewer::brewer.pal(7, "Purples")[3:7]) +
facet_wrap(~result, scales = "free", nrow = 1)
install.packages("tidyverse")
library(tidyverse)
library(shiny)
library(readr)
data <- read_csv("Downloads/Santander - OOH - General - Página1.csv")
View(data)
enriched_data <- data %>%
select(Player, Tipo, ID, Endereço, Lat, Lng)
enriched_data <- data %>%
select(Player, Tipo, ID, Endereço, Lat, Long)
data %>%
select(Player, Tipo, ID, Endereço, Lat, Long)
enriched_data %>% count()
enriched_data <- data %>%
select(Player, Tipo, ID, Endereço, Lat, Long) %>%
filter(!is.na(ID))
enriched_data %>% count()
enriched_data %>% group_by(Player) %>%  count()
data %>%
select(Player, Tipo, ID, Endereço, Lat, Long) %>%
filter(is.na(ID)) %>% group_by(Player) %>% count()
data %>% distinct(Player)
install.packages(c("Rcpp", "tidyr", "dplyr", "ggplot2", "microbenchmark", "knitr"))
install.packages(c("Rcpp", "tidyr", "dplyr", "ggplot2", "microbenchmark", "knitr"))
install.packages("tidyverse")
library(tidyverse)
big_matrix <- matrix(runif(0,1,1000))
big_matrix <- matrix(runif(0,1,900), nrow = 30, ncol = 30)
big_matrix
runif(0,1,900)
big_matrix <- matrix(runif(100, 0, 1), nrow = 30, ncol = 30)
big_matrix
View(big_matrix)
set.seed(1987)
big_matrix <- matrix(runif(100, 0, 1), nrow = 30, ncol = 30)
big_matrix
?sweap
??sweap
Base <- function(big_matrix, N) {
stopifnot(exists("big_matrix"))
stopifnot(exists("N"))
Time <- microbenchmark({
productMat <- big_matrix %*% big_matrix
}, times = N)$time
return(Time)
}
Base(big_matrix, N=2)
library(microbenchmark)
Base(big_matrix, N=2)
Base(big_matrix, N=100)
Base(big_matrix, N=1000)
setwd("~/Documents/code/intensive-comp/matricial-calculation-performance")
cat(functions/Base.R)
cat("functions/Base.R")
